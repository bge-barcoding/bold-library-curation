# Read configuration file
configfile: "config/config_optimized.yml"

# Helper function to get the appropriate input file based on filtering configuration
def get_input_file():
    """Return the appropriate input file based on whether filtering is enabled"""
    if config.get("ENABLE_PRESCORING_FILTER", False):
        return config["PRESCORING_FILTERED_OUTPUT"]
    else:
        return config["BOLD_TSV"]

# Helper function to determine the appropriate dependency for criteria assessment
def get_taxonomy_dependency():
    """Return appropriate dependency based on target list usage"""
    if config.get("USE_TARGET_LIST", False):
        return "results/target_loaded.ok"
    else:
        return "results/taxonomy_loaded.ok"

# Rule for pre-scoring filter (conditional execution)
if config.get("ENABLE_PRESCORING_FILTER", False):
    rule prescoring_filter:
        input:
            bold_tsv=config["BOLD_TSV"]
        output:
            filtered_file=config["PRESCORING_FILTERED_OUTPUT"],
            marker="results/prescoring_filter.ok"
        params:
            taxa_arg=lambda wildcards: f"--taxa-list {config['FILTER_TAXA_LIST']}" if config.get("FILTER_TAXA", False) and config.get("FILTER_TAXA_LIST") else "",
            country_arg=lambda wildcards: f"--country-list {config['FILTER_COUNTRY_LIST']}" if config.get("FILTER_COUNTRIES", False) and config.get("FILTER_COUNTRY_LIST") else "",
            marker_arg=lambda wildcards: f"--marker {config['MARKER']}" if config.get("MARKER") else "",
            bin_arg="--enable-bin-sharing" if config.get("FILTER_BINS", False) else "",
            log_level=config['LOG_LEVEL']
        log: "logs/prescoring_filter.log"
        conda: "envs/prescoring_filter.yaml"
        shell:
            """
            # Ensure output directory exists
            mkdir -p $(dirname {output.filtered_file})
            
            # Run the filtering script
            python workflow/scripts/prescoring_filter.py \
                --input {input.bold_tsv} \
                --output {output.filtered_file} \
                --log-level {params.log_level} \
                {params.taxa_arg} \
                {params.country_arg} \
                {params.marker_arg} \
                {params.bin_arg} \
                2> {log}
            
            # Create completion marker
            echo "Prescoring filter completed" > {output.marker}
            """
else:
    # Create a dummy rule when filtering is disabled
    rule skip_prescoring_filter:
        output:
            marker="results/prescoring_filter.ok"
        shell:
            """
            echo "Prescoring filter disabled - using original file" > {output.marker}
            """

# Rule for removing intermediate output
rule clean:
    shell:
        """
        perl workflow/scripts/clean.pl
        """

# Rule for creating the database and loading BCDM using fast_simple
rule create_load_db:
    input:
        schema=config["SCHEMA"],
        tsv_file=get_input_file(),  # Use dynamic input function
        prescoring_ok="results/prescoring_filter.ok"  # Explicit dependency
    output: config["DB_FILE"]
    params: log_level=config['LOG_LEVEL']
    log: "logs/create_load_db.log"
    conda: "envs/create_load_db.yaml"
    shell:
        "perl workflow/scripts/load_bcdm_fast_simple.pl \
            --tsv {input.tsv_file} \
            --db {output} \
            --sql {input.schema} \
            --log {params.log_level} \
            --force 2> {log}"

# Rule to load criteria
rule load_criteria:
    input:
        criteria="resources/criteria.tsv",
        db=config["DB_FILE"]
    output:
        "results/criteria_loaded.ok"
    log: "logs/load_criteria.log"
    conda: "envs/sqlite.yaml"
    shell:
        """
        sqlite3 {input.db} <<CRITERIA
.mode tabs
.import {input.criteria} criteria
.quit
CRITERIA
2> {log} && touch {output}
        """

# Rule for applying indexes to the database
rule apply_indexes:
    input:
        indexes=config["INDEXES"],
        db=config["DB_FILE"],
        criteria_ok="results/criteria_loaded.ok"
    output: config["DB_FILE_INDEXED"]
    log: "logs/apply_indexes.log"
    conda: "envs/sqlite.yaml"
    shell:
        """
        sqlite3 {input.db} < {input.indexes} 2> {log} && touch {output}
        """

# Rule for loading taxonomy into the database (UPDATED TO USE FASTER SCRIPT)
rule load_taxonomy:
    input:
        db=config["DB_FILE"],
        index_ok=config["DB_FILE_INDEXED"]
    output:
        "results/taxonomy_loaded.ok"
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        chunk_size=config.get("TAXONOMY_CHUNK_SIZE", 10000)  # Configurable chunk size with default
    log: "logs/load_taxonomy.log"
    conda: "envs/load_taxonomy.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/load_taxonomy_faster.pl \
            --db {input.db} \
            --log {params.log_level} \
            --chunk {params.chunk_size} \
            2> {log} && touch {output}
        """

# Conditional rule for importing target list (only runs if USE_TARGET_LIST is true)
if config.get("USE_TARGET_LIST", False):
    rule import_target_list:
        input:
            db=config["DB_FILE"],
            taxonomy_ok="results/taxonomy_loaded.ok",
            targetlist=config["TARGET_LIST"]
        output:
            "results/target_loaded.ok"
        params:
            log_level=config['LOG_LEVEL'],
            libs=config["LIBS"],
            project=config["PROJECT_NAME"],
            taxon=config["TAXON_LEVEL"],
            kingdom=config["KINGDOM"]
        log: "logs/load_target_list.log"
        conda: "envs/load_taxonomy.yaml"
        shell:
            """
            perl -I{params.libs} workflow/scripts/load_targetlist.pl \
                --list {input.targetlist} \
                --db {input.db} \
                --log {params.log_level} \
                --project {params.project} \
                --taxon {params.taxon} \
                --kingdom {params.kingdom} \
                2> {log} && touch {output}
            """
else:
    # Create a pass-through rule when target list is not used
    rule skip_target_list:
        input:
            "results/taxonomy_loaded.ok"
        output:
            "results/target_loaded.ok"
        shell:
            "cp {input} {output}"

# Rules for assessing criteria
# COLLECTION_DATE
rule COLLECTION_DATE:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="COLLECTION_DATE"
    output:
        tsv="results/assessed_COLLECTION_DATE.tsv"
    log: "logs/assess_COLLECTION_DATE.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# COLLECTORS
rule COLLECTORS:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="COLLECTORS"
    output:
        tsv="results/assessed_COLLECTORS.tsv"
    log: "logs/assess_COLLECTORS.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# COUNTRY
rule COUNTRY:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="COUNTRY"
    output:
        tsv="results/assessed_COUNTRY.tsv"
    log: "logs/assess_COUNTRY.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# ID_METHOD
rule ID_METHOD:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="ID_METHOD"
    output:
        tsv="results/assessed_ID_METHOD.tsv"
    log: "logs/assess_ID_METHOD.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# IDENTIFIER
rule IDENTIFIER:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="IDENTIFIER"
    output:
        tsv="results/assessed_IDENTIFIER.tsv"
    log: "logs/assess_IDENTIFIER.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# INSTITUTION
rule INSTITUTION:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="INSTITUTION"
    output:
        tsv="results/assessed_INSTITUTION.tsv"
    log: "logs/assess_INSTITUTION.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# COORD
rule COORD:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="COORD"
    output:
        tsv="results/assessed_COORD.tsv"
    log: "logs/assess_COORD.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# MUSEUM_ID
rule MUSEUM_ID:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="MUSEUM_ID"
    output:
        tsv="results/assessed_MUSEUM_ID.tsv"
    log: "logs/assess_MUSEUM_ID.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# PUBLIC_VOUCHER
rule PUBLIC_VOUCHER:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="PUBLIC_VOUCHER"
    output:
        tsv="results/assessed_PUBLIC_VOUCHER.tsv"
    log: "logs/assess_PUBLIC_VOUCHER.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# SEQ_QUALITY
rule SEQ_QUALITY:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="SEQ_QUALITY"
    output:
        tsv="results/assessed_SEQ_QUALITY.tsv"
    log: "logs/assess_SEQ_QUALITY.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# SITE
rule SITE:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="SITE"
    output:
        tsv="results/assessed_SITE.tsv"
    log: "logs/assess_SITE.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# REGION
rule REGION:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="REGION"
    output:
        tsv="results/assessed_REGION.tsv"
    log: "logs/assess_REGION.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# SECTOR
rule SECTOR:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="SECTOR"
    output:
        tsv="results/assessed_SECTOR.tsv"
    log: "logs/assess_SECTOR.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# SPECIES_ID
rule SPECIES_ID:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="SPECIES_ID"
    output:
        tsv="results/assessed_SPECIES_ID.tsv"
    log: "logs/assess_SPECIES_ID.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# TYPE_SPECIMEN
rule TYPE_SPECIMEN:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"],
        criterion="TYPE_SPECIMEN"
    output:
        tsv="results/assessed_TYPE_SPECIMEN.tsv"
    log: "logs/assess_TYPE_SPECIMEN.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_criteria.pl \
            --db {input.db} \
            --log {params.log_level} \
            --criteria {params.criterion} \
            2> {log} > {output.tsv}
        """

# HAS_IMAGE
rule HAS_IMAGE:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"]
    output:
        tsv="results/assessed_HAS_IMAGE.tsv"
    log: "logs/assess_HAS_IMAGE.log"
    conda: "envs/assess_images.yaml"
    shell:
        """
        perl -I{params.libs} workflow/scripts/assess_images.pl \
            --db {input.db} \
            --log {params.log_level} \
            2> {log} > {output.tsv}
        """

# PHASE 1 OPTIMIZATION: Apply BAGS-specific database optimizations
rule optimize_bags_database:
    input:
        db=config["DB_FILE"],
        taxonomy_ok=get_taxonomy_dependency()
    output:
        "results/bags_optimized.ok"
    log: "logs/optimize_bags_database.log"
    conda: "envs/sqlite.yaml"
    shell:
        """
        echo "Applying BAGS-specific database optimizations..." > {log}
        
        # Apply BAGS-specific indexes and optimizations
        sqlite3 {input.db} 2>> {log} <<OPTIMIZE
-- BAGS Performance Optimizations (Phase 1)
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = -64000;
PRAGMA temp_store = MEMORY;

-- Apply BAGS-specific indexes
.read workflow/scripts/bags_indexes.sql

-- Update query planner statistics
ANALYZE bold;
ANALYZE taxa;

-- Final optimization
PRAGMA optimize;
.quit
OPTIMIZE

        echo "BAGS database optimization completed on $(date)" >> {log}
        touch {output}
        """

# BAGS (species-level assessment) - SIMPLIFIED WITH CLEAN PROGRESS TRACKING
rule BAGS:
    input:
        db=config["DB_FILE"],
        bags_optimized="results/bags_optimized.ok"
    params:
        log_level=config['LOG_LEVEL'],
        libs=config["LIBS"]
    output:
        tsv="results/assessed_BAGS.tsv"
    log: "logs/assess_BAGS.log"
    conda: "envs/assess_criteria.yaml"
    shell:
        """
        echo "=== BAGS Analysis Started ===" > {log}
        echo "Start time: $(date)" >> {log}
        echo "Database: {input.db}" >> {log}
        echo "Using simplified progress tracking for clear monitoring" >> {log}
        echo "" >> {log}
        
        # Run BAGS analysis with filtered progress output
        echo "Starting BAGS analysis with filtered progress reporting..." >> {log}
        echo "Debug output will be present but progress clearly marked" >> {log}
        echo "" >> {log}
        
        # Run analysis and filter for progress lines
        perl -I{params.libs} workflow/scripts/assess_taxa_filtered.pl \
            --db {input.db} \
            --progress 50 \
            > {output.tsv} \
            2> >(tee logs/bags_full_debug.log | grep "PROGRESS:" >> {log}) || \
        # Fallback for systems without process substitution
        (perl -I{params.libs} workflow/scripts/assess_taxa_filtered.pl \
            --db {input.db} \
            --progress 50 \
            > {output.tsv} 2> logs/bags_all_output.log && \
         echo "Extracting progress information..." >> {log} && \
         grep "PROGRESS:" logs/bags_all_output.log >> {log} || true)
        
        echo "" >> {log}
        echo "=== BAGS Analysis Completed ===" >> {log}
        echo "End time: $(date)" >> {log}
        
        # Generate summary statistics
        if [ -f {output.tsv} ]; then
            TOTAL_RECORDS=$(tail -n +2 {output.tsv} | wc -l)
            UNIQUE_SPECIES=$(tail -n +2 {output.tsv} | cut -f5 | sort -u | wc -l)
            echo "Summary:" >> {log}
            echo "  Total records: $TOTAL_RECORDS" >> {log}
            echo "  Unique species: $UNIQUE_SPECIES" >> {log}
            echo "" >> {log}
            echo "Grade distribution:" >> {log}
            tail -n +2 {output.tsv} | cut -f6 | sort | uniq -c | sort -rn | while read count grade; do
                echo "  Grade $grade: $count records" >> {log}
            done
        fi
        
        echo "" >> {log}
        echo "Output file: {output.tsv}" >> {log}
        """

# Modified rule to import BAGS data into database
rule import_bags:
    input:
        bags_tsv="results/assessed_BAGS.tsv",
        db=config["DB_FILE"]
    output:
        "results/bags_imported.ok"
    conda: "envs/sqlite.yaml"
    log: "logs/import_bags.log"
    shell:
        """
        sqlite3 {input.db} 2> {log} <<BAGS
CREATE TABLE IF NOT EXISTS bags (
    taxonid INTEGER,
    order_name TEXT,
    family_name TEXT,
    genus_name TEXT,
    species_name TEXT,
    bags_grade TEXT,
    bin_uri TEXT,
    sharers TEXT
);
.mode tabs
.import {input.bags_tsv} bags_temp
INSERT INTO bags SELECT * FROM bags_temp WHERE taxonid != 'taxonid';
DROP TABLE bags_temp;
.quit
BAGS
touch {output}
        """

# Rule to inherit BAGS grades for subspecies from their parent species
# This ensures subspecies get the same BAGS grade as their parent species
# Implementation: Post-processing approach that queries taxa hierarchy
rule inherit_subspecies_bags:
    input:
        db=config["DB_FILE"],
        bags_ok="results/bags_imported.ok"
    output:
        "results/subspecies_bags_inherited.ok"
    conda: "envs/sqlite.yaml"
    log: "logs/inherit_subspecies_bags.log"
    shell:
        """
        echo "Inheriting BAGS grades for subspecies from parent species..." > {log}
        
        sqlite3 {input.db} 2>> {log} <<INHERIT
-- Insert subspecies records with inherited BAGS grades from parent species
INSERT INTO bags (taxonid, order_name, family_name, genus_name, species_name, bags_grade, bin_uri, sharers)
SELECT 
    s.taxonid,
    b.order_name,
    b.family_name, 
    b.genus_name,
    s.name as species_name,
    b.bags_grade,
    b.bin_uri,
    b.sharers
FROM taxa s
JOIN taxa parent ON s.parent_taxonid = parent.taxonid
JOIN bags b ON parent.taxonid = b.taxonid
WHERE s.level = 'subspecies'
  AND parent.level = 'species'
  AND s.taxonid NOT IN (SELECT taxonid FROM bags WHERE taxonid = s.taxonid);

-- Log subspecies inheritance statistics
SELECT 'Subspecies BAGS inheritance completed: ' || COUNT(*) || ' subspecies inherited grades' as result
FROM taxa s
JOIN taxa parent ON s.parent_taxonid = parent.taxonid  
JOIN bags b ON parent.taxonid = b.taxonid
WHERE s.level = 'subspecies' AND parent.level = 'species';
.quit
INHERIT

        echo "Subspecies BAGS inheritance completed on $(date)" >> {log}
        touch {output}
        """

rule concatenate:
    input:
        collection_date = rules.COLLECTION_DATE.output.tsv,
        collectors = rules.COLLECTORS.output.tsv,
        coord = rules.COORD.output.tsv,
        country = rules.COUNTRY.output.tsv,
        id_method = rules.ID_METHOD.output.tsv,
        identifier = rules.IDENTIFIER.output.tsv,
        institution = rules.INSTITUTION.output.tsv,
        museum_id = rules.MUSEUM_ID.output.tsv,
        public_voucher = rules.PUBLIC_VOUCHER.output.tsv,
        seq_quality = rules.SEQ_QUALITY.output.tsv,
        site = rules.SITE.output.tsv,
        region = rules.REGION.output.tsv,
        sector = rules.SECTOR.output.tsv,
        species_id = rules.SPECIES_ID.output.tsv,
        type_specimen = rules.TYPE_SPECIMEN.output.tsv,
        has_image = rules.HAS_IMAGE.output.tsv
    output:
        concat="results/CONCATENATED.tsv"
    shell:
        """
        perl workflow/scripts/concat_tsvs.pl \
            {input.collection_date} \
            {input.collectors} \
            {input.coord} \
            {input.country} \
            {input.id_method} \
            {input.identifier} \
            {input.institution} \
            {input.museum_id} \
            {input.public_voucher} \
            {input.seq_quality} \
            {input.site} \
            {input.region} \
            {input.sector} \
            {input.species_id} \
            {input.type_specimen} \
            {input.has_image} \
            > {output.concat}
        """

# Updated import_concatenated rule to depend on BAGS import
rule import_concatenated:
    input:
        concat="results/CONCATENATED.tsv",
        db=config["DB_FILE"],
        subspecies_ok="results/subspecies_bags_inherited.ok"  # Updated dependency
    output:
        "results/concatenated_imported.ok"
    conda: "envs/sqlite.yaml"
    log: "logs/import_concatenated.log"
    shell:
        """
sqlite3 {input.db} 2> {log} <<IMPORT
.mode tabs
.import {input.concat} bold_criteria
.quit
IMPORT
touch {output}
        """

# Rule for outputting filtered data with ranking and sumscore        
rule output_filtered_data:
    input:
        db=config["DB_FILE"],
        import_ok="results/concatenated_imported.ok"
    output:
        "results/result_output.tsv"
    conda: "envs/sqlite.yaml"
    log: "logs/output_filtered_data.log"
    shell:
        """
        sqlite3 {input.db} 2> {log} <<EOF
.headers ON        
.mode tabs
.output {output}
.read workflow/scripts/ranking_with_sumscore.sql
.quit
EOF
        """

# Rule for splitting database into family-level databases
rule split_families:
    input:
        result_tsv="results/result_output.tsv",
        db=config["DB_FILE"]
    output:
        marker="results/families_split.ok",
        report="results/family_databases/splitting_report.txt"
    params:
        threshold=config.get("FAMILY_SIZE_THRESHOLD", 10000),
        output_dir="results/family_databases",
        script_path="workflow/scripts/bold_family_splitter.js"
    log: "logs/split_families.log"
    conda: "envs/family_splitter.yaml"
    shell:
        """
        echo "Starting family database splitting..." > {log}
        echo "Input TSV: {input.result_tsv}" >> {log}
        echo "Input DB: {input.db}" >> {log}
        echo "Threshold: {params.threshold}" >> {log}
        echo "Output directory: {params.output_dir}" >> {log}
        echo "" >> {log}
        
        # Create output directory
        mkdir -p {params.output_dir}
        
        # Run the family splitter (try TSV first, fallback to DB)
        if [ -f "{input.result_tsv}" ] && [ -s "{input.result_tsv}" ]; then
            echo "Using TSV input: {input.result_tsv}" >> {log}
            node {params.script_path} "{input.result_tsv}" "{params.output_dir}" {params.threshold} 2>> {log}
        elif [ -f "{input.db}" ]; then
            echo "Using database input: {input.db}" >> {log}
            node {params.script_path} "{input.db}" "{params.output_dir}" {params.threshold} 2>> {log}
        else
            echo "ERROR: No valid input found" >> {log}
            exit 1
        fi
        
        # Verify output
        if [ -f "{output.report}" ]; then
            echo "Family splitting completed successfully" >> {log}
            echo "Report generated: {output.report}" >> {log}
            
            # Log summary statistics
            echo "" >> {log}
            echo "=== Summary ===" >> {log}
            DB_COUNT=$(find {params.output_dir} -name "*.db" | wc -l)
            echo "Total family databases created: $DB_COUNT" >> {log}
            
            # Show directory structure
            echo "" >> {log}
            echo "Output structure:" >> {log}
            find {params.output_dir} -type d | head -20 | while read dir; do
                echo "  $dir" >> {log}
            done
            
            touch {output.marker}
        else
            echo "ERROR: Family splitting failed - no report generated" >> {log}
            exit 1
        fi
        """

# Updated final rule to include family splitting AND final summary
rule all:
    input:
        "results/result_output.tsv",
        "results/families_split.ok",
        "results/pipeline_summary.txt"  # Add the final summary
    default_target: True

# Optional rule to create a summary of all outputs
rule create_final_summary:
    input:
        result_tsv="results/result_output.tsv",
        families_split="results/families_split.ok",
        split_report="results/family_databases/splitting_report.txt"
    output:
        summary="results/pipeline_summary.txt"
    shell:
        """
        echo "BOLD Library Curation Pipeline - Final Summary" > {output.summary}
        echo "=============================================" >> {output.summary}
        echo "Completed: $(date)" >> {output.summary}
        echo "" >> {output.summary}
        
        # Main results
        echo "Main Results:" >> {output.summary}
        echo "- Processed database: results/bold.db" >> {output.summary}
        echo "- Final scored data: {input.result_tsv}" >> {output.summary}
        if [ -f "{input.result_tsv}" ]; then
            TOTAL_RECORDS=$(tail -n +2 {input.result_tsv} | wc -l)
            echo "- Total records: $TOTAL_RECORDS" >> {output.summary}
        fi
        echo "" >> {output.summary}
        
        # Family databases
        echo "Family Databases:" >> {output.summary}
        if [ -f "{input.split_report}" ]; then
            grep -A 20 "Family Statistics:" {input.split_report} | head -20 >> {output.summary}
        fi
        echo "" >> {output.summary}
        
        # File structure
        echo "Output Structure:" >> {output.summary}
        echo "results/" >> {output.summary}
        echo "├── bold.db                    # Main database" >> {output.summary}
        echo "├── result_output.tsv          # Final scored data" >> {output.summary}
        echo "├── assessed_*.tsv             # Individual criteria assessments" >> {output.summary}
        echo "└── family_databases/          # Split by taxonomy" >> {output.summary}
        
        DB_COUNT=$(find results/family_databases -name "*.db" 2>/dev/null | wc -l)
        echo "    ├── $DB_COUNT family/subfamily databases" >> {output.summary}
        
        PHYLA_COUNT=$(find results/family_databases -maxdepth 1 -type d 2>/dev/null | wc -l)
        echo "    └── organized in $PHYLA_COUNT phyla" >> {output.summary}
        
        echo "" >> {output.summary}
        echo "For detailed family splitting report, see: {input.split_report}" >> {output.summary}
        """