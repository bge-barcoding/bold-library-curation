# Output directory configuration
RESULTS_DIR: "results"  # Default results directory - can be overridden
LOG_DIR: "logs"  # Default log directory - can be overridden

# Input data configuration
BOLD_TSV: resources/test_data/test_odonata_large.tsv
SCHEMA: workflow/scripts/schema_with_ranks.sql
INDEXES: workflow/scripts/indexes.sql

# Library and path configuration
LIBS: lib/
PATH: lib/

# Processing configuration
TAXONOMY_CHUNK_SIZE: 10000  # Adjust based on your system's memory
CRITERIA: "COLLECTION_DATE COLLECTORS COORD COUNTRY REGION SECTOR HAS_IMAGE IDENTIFIER ID_METHOD INSTITUTION MUSEUM_ID PUBLIC_VOUCHER SEQ_QUALITY SITE SPECIES_ID TYPE_SPECIMEN"

# Target list configuration
TARGET_LIST: resources/test_data/test_odonata_spec.csv
USE_TARGET_LIST: false
PROJECT_NAME: "bold-curation"
TAXON_LEVEL: "species"
KINGDOM: "Animalia"

# Logging configuration
LOG_LEVEL: "INFO"

# Pre-scoring filter options
ENABLE_PRESCORING_FILTER: true  # Master switch true / false
FILTER_TAXA: true  # true / false
FILTER_TAXA_LIST: "resources/test_data/test_odonata_spec.csv"
FILTER_COUNTRIES: true  # true / false
FILTER_COUNTRY_LIST: "resources/countries/test_odonata.txt"
FILTER_BINS: true  # Include BIN_URI sharing true / false
FILTER_SPECIES: true  # Only include records with valid species names (not null, empty, or "None") true / false
MARKER: "COI-5P"  # Specify marker code (e.g., "COI-5P") to filter by marker_code field, or null to disable marker filtering
PRESCORING_FILTERED_OUTPUT: "test_odonata_large_filtered.tsv"  # Filename only - will be placed in RESULTS_DIR

# OTU Clustering configuration
OTU_CLUSTERING_THRESHOLD: 0.99  # Similarity threshold (99%)
OTU_CLUSTERING_THREADS: 8       # Number of threads for VSEARCH

# -------------------------------
# Family splitting configuration
# -------------------------------
# Family size threshold for subfamily splitting (default: 10000)
FAMILY_SIZE_THRESHOLD: 10000 # Split families larger than this by subfamily

# SLURM Job Array Configuration
# Number of parallel jobs in the array (default: 64)
FAMILY_ARRAY_SIZE: 64

# Number of worker threads per job (default: 4)
# Total cores used = FAMILY_ARRAY_SIZE * WORKERS_PER_JOB
WORKERS_PER_JOB: 4

# SLURM Resource Allocation per Job
# Memory per job (default: 8G)
JOB_MEMORY: "8G"

# Maximum time per job (default: 4 hours)
JOB_TIME: "04:00:00"

# Compression settings
COMPRESSION_WORKERS: 16  # Adjust based on your HPC node specs
COMPRESS_AFTER_SPLIT: true  # Enable/disable compression
REMOVE_ORIGINAL_DBS: false  # Keep originals for safety initially

# Note: Output paths are dynamically constructed using RESULTS_DIR and LOG_DIR 
# in the Snakefile helper functions. Directories are created automatically.